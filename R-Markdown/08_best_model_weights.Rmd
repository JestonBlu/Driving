---
title: "Examining Best Model Weights"
author: "Joseph Blubaugh"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document:
    latex_engine: xelatex
    highlight: pygments
mainfont: DejaVu Sans Mono
sansfont: DejaVu Sans Mono
fontsize: 11pt
geometry: margin=1in
---

```{r a, echo=FALSE}
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(gridExtra))
suppressPackageStartupMessages(library(plyr))
suppressPackageStartupMessages(library(reshape2))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(pander))
suppressPackageStartupMessages(library(nnet))

## Model Performance Metric
metric = function(confusion) {
  sensitivity = confusion[4] / (confusion[2] + confusion[4])
  specificity = confusion[1] / (confusion[1] + confusion[3])
  score = (sensitivity + specificity) / 2
  return(score)
}

```

## Overview

The objective of this project is to develop a model capable of detecting texting events from facial expressions. From the previous update the best model is Model 08 which used data segmented into 1/2 second intervals and computed the mean value for each facial expression over the entire interval (15 observations). The goal of this analysis is to extract and analyze the calculated weights in the current best model.

```{r b, echo=FALSE, comment=NA}
load("../R-Data/data-mdl-08.rda")
load("../R-Models/mdl_08_nnet.rda")

x = predict(mdl.08, mdl.08.train, type = "raw")
y = predict(mdl.08, mdl.08.test, type = "raw")

cat("------------------------------------------------------------------")
cat("Neural Network Confusion Matrix (Best Model)")
cat("-------------------------------------------------------------------")
table(Actual = mdl.08.train$Texting, Predicted = x)
cat("(Training Set) Overall Performance:", metric(table(Actual = mdl.08.train$Texting, Predicted = x)))
table(Actual = mdl.08.test$Texting, Predicted = y)
cat("(Testing Set) Neural Net Overall Performance", metric(table(Actual = mdl.08.test$Texting, Predicted = y)))
```

## Model Weights

In neural net models, weights are the interconnection between Input_Node -> Hidden_Layer, and Hidden_Layer -> Output_Node. They are analogous to coefficients in regression models. The sum product of the weights and input nodes generates the model output after going through a transformation (activation function). In the case of my current logistic regression setup, the activation function is the sigmoid function (shown in a previous report). In the current best model there are (68 Input Nodes * 100 Hidden Nodes) + (1 Bias Correction * 100 Hidden Nodes) + (100 Hidden Nodes * 1 Output Node) + (1 Output Bias Correction) = 7001 Weights.

| Input Nodes          | Hidden Nodes      | Output Nodes      |
|:---------------------|:------------------|:------------------|
| 58 Subjects (1 Base) | 100               | 1                 |
| 2 Demographics       | 1 Bias Correction | 1 Bias Correction |
| 8 Emotions           |                   |                   |

\newpage

Neural nets do not produce standard errors, tvalues, or pvalues like traditional regression models do for coefficients. In fact neural networks are not really intended to be looked at under the hood. Most of the diagnostics for neural nets rely on examining the differences between training and testing set performance differences. That being said the hidden nodes within a neural net can be thought of as individual logistic regression models. You can extract the weights between the input nodes and the hidden layers and examine the distribution of weights for common indicator variables like age, gender, and subject.

```{r c, echo=FALSE, comment=NA, warning=FALSE, message=FALSE}

s1.pos = seq(from = 38, to = 6900, by = 69)
s2.pos = seq(from = 2, to = 6900, by = 69)
s3.pos = seq(from = 3, to = 6900, by = 69)
s4.pos = seq(from = 22, to = 6900, by = 69)

subject1 = mdl.08$finalModel$wts[s1.pos]
subject2 = mdl.08$finalModel$wts[s2.pos]
subject3 = mdl.08$finalModel$wts[s3.pos]
subject4 = mdl.08$finalModel$wts[s4.pos]

subjects = data.frame(subject1, subject2, subject3, subject4)


g1 = ggplot(subjects) +
  geom_histogram(aes(x = subject1)) +
  scale_x_continuous("weights") +
  ggtitle("Subject 38 (51% Accuracy)") +
  theme(plot.title = element_text(hjust = 0.5))

g2 = ggplot(subjects) +
  geom_histogram(aes(x = subject2)) +
  scale_x_continuous("weights") +
  ggtitle("Subject 2 (71% Accuracy)") +
  theme(plot.title = element_text(hjust = 0.5))

g3 = ggplot(subjects) +
  geom_histogram(aes(x = subject3)) +
  scale_x_continuous("weights") +
  ggtitle("Subject 3 (88% Accuracy)") +
  theme(plot.title = element_text(hjust = 0.5))

g4 = ggplot(subjects) +
  geom_histogram(aes(x = subject4)) +
  scale_x_continuous("weights") +
  ggtitle("Subject 22 (97% Accuracy)") +
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(g1, g2, g3, g4, nrow = 2)

```

|        | Subject 38 | Subject 2 | Subject 3 | Subject 22 |
|:-------|:-----------|:----------|:----------|:-----------|
| Min    | -3.135314  | -3.00098  | -2.35483  | -4.069946  |
| 1st Q  | -0.337495  | -0.33404  | -0.66465  | -0.354756  |
| Median | 0.009406   | 0.03642   | -0.04398  | -0.007874  |
| Mean   | 0.005288   | 0.08909   | -0.06727  | -0.062273  |
| 3rd Q  | 0.331374   | 0.60957   | 0.24815   | 0.257508   |
| Max    | 3.464602   | 3.19086   | 2.97495   | 3.961502   |

Looking at a sample of subject weights which includes the worst (subject 38) and the best (subject 22) subjects in terms of model accuracy, there appears to be no real distinction between the weights of the subjects. It turns out that this is a trait of neural networks. Although the hidden nodes can be considered individual models, they are in fact linked in that weights are adjusted based on their combined predictive error. This makes it difficult to infere anything about the weights when you have multiple nodes in a hidden layer.

\newpage

```{r d, echo=FALSE, comment=NA, warning=FALSE, message=FALSE, fig.height=3}

gar.fun<-function(out.var,mod.in,bar.plot=T,struct=NULL,x.lab=NULL,
                  y.lab=NULL, wts.only = F){

  require(ggplot2)
  require(plyr)

  # function works with neural networks from neuralnet, nnet, and RSNNS package
  # manual input vector of weights also okay

  #sanity checks
  if('numeric' %in% class(mod.in)){
    if(is.null(struct)) stop('Three-element vector required for struct')
    if(length(mod.in) != ((struct[1]*struct[2]+struct[2]*struct[3])+(struct[3]+struct[2])))
      stop('Incorrect length of weight matrix for given network structure')
    if(substr(out.var,1,1) != 'Y' |
       class(as.numeric(gsub('^[A-Z]','', out.var))) != 'numeric')
      stop('out.var must be of form "Y1", "Y2", etc.')
  }
  if('train' %in% class(mod.in)){
    if('nnet' %in% class(mod.in$finalModel)){
      mod.in<-mod.in$finalModel
      warning('Using best nnet model from train output')
    }
    else stop('Only nnet method can be used with train object')
  }

  #gets weights for neural network, output is list
  #if rescaled argument is true, weights are returned but rescaled based on abs value
  nnet.vals<-function(mod.in,nid,rel.rsc,struct.out=struct){

    require(scales)
    require(reshape)

    if('numeric' %in% class(mod.in)){
      struct.out<-struct
      wts<-mod.in
    }

    #neuralnet package
    if('nn' %in% class(mod.in)){
      struct.out<-unlist(lapply(mod.in$weights[[1]],ncol))
      struct.out<-struct.out[-length(struct.out)]
      struct.out<-c(
        length(mod.in$model.list$variables),
        struct.out,
        length(mod.in$model.list$response)
      )
      wts<-unlist(mod.in$weights[[1]])
    }

    #nnet package
    if('nnet' %in% class(mod.in)){
      struct.out<-mod.in$n
      wts<-mod.in$wts
    }

    #RSNNS package
    if('mlp' %in% class(mod.in)){
      struct.out<-c(mod.in$nInputs,mod.in$archParams$size,mod.in$nOutputs)
      hid.num<-length(struct.out)-2
      wts<-mod.in$snnsObject$getCompleteWeightMatrix()

      #get all input-hidden and hidden-hidden wts
      inps<-wts[grep('Input',row.names(wts)),grep('Hidden_2',colnames(wts)),drop=F]
      inps<-melt(rbind(rep(NA,ncol(inps)),inps))$value
      uni.hids<-paste0('Hidden_',1+seq(1,hid.num))
      for(i in 1:length(uni.hids)){
        if(is.na(uni.hids[i+1])) break
        tmp<-wts[grep(uni.hids[i],rownames(wts)),grep(uni.hids[i+1],colnames(wts)),drop=F]
        inps<-c(inps,melt(rbind(rep(NA,ncol(tmp)),tmp))$value)
      }

      #get connections from last hidden to output layers
      outs<-wts[grep(paste0('Hidden_',hid.num+1),row.names(wts)),grep('Output',colnames(wts)),drop=F]
      outs<-rbind(rep(NA,ncol(outs)),outs)

      #weight vector for all
      wts<-c(inps,melt(outs)$value)
      assign('bias',F,envir=environment(nnet.vals))
    }

    if(nid) wts<-rescale(abs(wts),c(1,rel.rsc))

    #convert wts to list with appropriate names
    hid.struct<-struct.out[-c(length(struct.out))]
    row.nms<-NULL
    for(i in 1:length(hid.struct)){
      if(is.na(hid.struct[i+1])) break
      row.nms<-c(row.nms,rep(paste('hidden',i,seq(1:hid.struct[i+1])),each=1+hid.struct[i]))
    }
    row.nms<-c(
      row.nms,
      rep(paste('out',seq(1:struct.out[length(struct.out)])),each=1+struct.out[length(struct.out)-1])
    )
    out.ls<-data.frame(wts,row.nms)
    out.ls$row.nms<-factor(row.nms,levels=unique(row.nms),labels=unique(row.nms))
    out.ls<-split(out.ls$wts,f=out.ls$row.nms)

    assign('struct',struct.out,envir=environment(nnet.vals))

    out.ls

  }

  # get model weights
  best.wts<-nnet.vals(mod.in,nid=F,rel.rsc=5,struct.out=NULL)

  # weights only if T
  if(wts.only) return(best.wts)

  #get variable names from mod.in object
  #change to user input if supplied
  if('numeric' %in% class(mod.in)){
    x.names<-paste0(rep('X',struct[1]),seq(1:struct[1]))
    y.names<-paste0(rep('Y',struct[3]),seq(1:struct[3]))
  }
  if('mlp' %in% class(mod.in)){
    all.names<-mod.in$snnsObject$getUnitDefinitions()
    x.names<-all.names[grep('Input',all.names$unitName),'unitName']
    y.names<-all.names[grep('Output',all.names$unitName),'unitName']
  }
  if('nn' %in% class(mod.in)){
    x.names<-mod.in$model.list$variables
    y.names<-mod.in$model.list$response
  }
  if('xNames' %in% names(mod.in)){
    x.names<-mod.in$xNames
    y.names<-attr(terms(mod.in),'factor')
    y.names<-row.names(y.names)[!row.names(y.names) %in% x.names]
  }
  if(!'xNames' %in% names(mod.in) & 'nnet' %in% class(mod.in)){
    if(is.null(mod.in$call$formula)){
      x.names<-colnames(eval(mod.in$call$x))
      y.names<-colnames(eval(mod.in$call$y))
    }
    else{
      forms<-eval(mod.in$call$formula)
      x.names<-mod.in$coefnames
      facts<-attr(terms(mod.in),'factors')
      y.check<-mod.in$fitted
      if(ncol(y.check)>1) y.names<-colnames(y.check)
      else y.names<-as.character(forms)[2]
    }
  }

  # get index value for response variable to measure
  if('numeric' %in% class(mod.in)){
    out.ind <-  as.numeric(gsub('^[A-Z]','',out.var))
  } else {
    out.ind<- grep(out.var, y.names)
  }

  #change variables names to user sub
  if(!is.null(x.lab)){
    if(length(x.names) != length(x.lab)) stop('x.lab length not equal to number of input variables')
    else x.names<-x.lab
  }
  if(!is.null(y.lab)){
    y.names<-y.lab
  } else {
    y.names <- y.names[grep(out.var, y.names)]
  }

  # organize hidden layer weights for matrix mult
  inp.hid <- best.wts[grep('hidden', names(best.wts))]
  split_vals <- substr(names(inp.hid), 1, 8)
  inp.hid <- split(inp.hid, split_vals)
  inp.hid <- lapply(inp.hid, function(x) t(do.call('rbind', x))[-1, ])

  # final layer weights for output
  hid.out<-best.wts[[grep(paste('out',out.ind),names(best.wts))]][-1]

  # matrix multiplication of output layer with connecting hidden layer
  max_i <- length(inp.hid)
  sum_in <- as.matrix(inp.hid[[max_i]]) %*% matrix(hid.out)

  # recursive matrix multiplication for all remaining hidden layers
  # only for multiple hidden layers
  if(max_i != 1){

    for(i in (max_i - 1):1) sum_in <- as.matrix(inp.hid[[i]]) %*% sum_in

    # final contribution vector for all inputs
    inp.cont <- sum_in

  } else {

    inp.cont <- sum_in

  }

  #get relative contribution
  #inp.cont/sum(inp.cont)
  rel.imp<-{
    signs<-sign(inp.cont)
    signs*rescale(abs(inp.cont),c(0,1))
  }

  if(!bar.plot){
    out <- data.frame(rel.imp)
    row.names(out) <- x.names
    return(out)
  }

  to_plo <- data.frame(rel.imp,x.names)[order(rel.imp),,drop = F]
  to_plo$x.names <- factor(x.names[order(rel.imp)], levels = x.names[order(rel.imp)])
  out_plo <- ggplot(to_plo, aes(x = x.names, y = rel.imp, fill = rel.imp,
                                colour = rel.imp)) +
    geom_bar(stat = 'identity') +
    scale_x_discrete(element_blank()) +
    scale_y_continuous(y.names)

  return(out_plo)

}

rel.imp = gar.fun(out.var = "Texting", mod.in = mdl.08$finalModel, bar.plot = FALSE)
rel.imp$Subject = row.names(rel.imp)
rel.imp$Subject = gsub(pattern = "Subject", replacement = "", x = rel.imp$Subject)

mdl.08.train$Predict = predict(mdl.08, mdl.08.train, type = "raw")
mdl.08.test$Predict = predict(mdl.08, mdl.08.test, type = "raw")

subject = as.character(unique(mdl.08.train$Subject))

tab = data.frame()

for (i in 1:59) {
  y1 = subset(mdl.08.train, Subject == subject[i])
  y2 = subset(mdl.08.test, Subject == subject[i])

  x1 = metric(table(Actual = y1$Texting, Predicted = y1$Predict))
  x2 = metric(table(Actual = y2$Texting, Predicted = y2$Predict))

  tab = rbind(tab, data.frame(Subject = subject[i], Train = x1, Test = x2))

}

tab = arrange(tab, desc(Test))
tab$Train = round(tab$Train, 3)
tab$Test = round(tab$Test, 3)
subject = as.character(tab$Subject)
rel.imp2 = na.omit(join(rel.imp, tab))
```

## Variable Importance

One way of assessing variable importance is to look at the sum of weights between the input nodes and the hidden nodes. The following plot shows the relative importance of for the 10 predictor variables (Subject not shown) scaled between -1 and 1.

```{r e, echo=FALSE, comment=NA, warning=FALSE, message=FALSE, fig.height=3}
x = arrange(rel.imp[-(grep("T", x = rel.imp$Subject)), ], desc(rel.imp))

ggplot(x, aes(x = reorder(Subject, rel.imp), y = rel.imp)) +
  geom_bar(stat = 'identity') +
  scale_x_discrete("Predictor Variables") +
  scale_y_continuous("Relative Importance") +
  coord_flip() +
  ggtitle("Predictor Variable Relative Importance") +
  theme(plot.title = element_text(hjust = .5))

```

The Neutral emotion has the highest importance, followed by Surprise, Anger, and Gender. Contempt has the lowest importance of all predictors including Subject. The Surprise emotion is a bit surprising that it is the second most important variable because we have seen very little variation in this emotion in previous plots compared to the other emotions. This should be investigated further.

It is also worth looking at the relative importance of the Subject variable since model performance increased significantly when it was added. The importance of Subject is broken out into each factor level. Relative Importance ranges from -.39 to .25. I was interested in seeing how realtive importance and model accuracy are related. There apprears to be some weak correlation between the two.

```{r f, echo=FALSE, comment=NA, warning=FALSE, message=FALSE, fig.height=3}
rel.imp3 = arrange(rel.imp2, rel.imp)
rel.imp3 = rel.imp3[-c(1, 2, 4, 37, 56, 58), ]

ggplot(rel.imp2, aes(x = rel.imp, y = Test)) +
  geom_point(color = "red") +
  geom_point(aes(x = rel.imp, y = Test), color = "black", data = rel.imp3) +
  geom_smooth(se = FALSE, method = "lm", color = "black", size = .25) +
  stat_ellipse(lty = 2) +
  annotate(x = -.35, y = .50, geom = "text", label = "Correlation (All Points): -.25", hjust = 0) +
  annotate(x = -.35, y = .45, geom = "text", label = "Correlation (No Red):   -.16", hjust = 0) +
  scale_x_continuous("Relative Imporance") +
  scale_y_continuous("Testing Set Accuracy") +
  ggtitle("Relative Importance vs Model Accuracy by Subject\n(--- 95% Confidence Ellipse)") +
  theme(plot.title = element_text(hjust = .5))

```

\newpage

## Emotions Effect on Texting Probability

The following boxplots show the probability of a texting event for each Subject as emotions change. Emotions are represented in intervals of .1 with 0 representing the baseline value of the simulation for that particular Subject. Only the variable on each x-axis is allowed to change, the rest are held constant at the baseline. The current best model was used to predict the probability of a texting event given the range of inputs. There are 59 values for each boxplot, representing the 59 subjects in the simulation.


```{r g, echo=FALSE, comment=NA, warning=FALSE, message=FALSE, fig.height=7, fig.width=7}

## Neutral
x = unique(mdl.08.train[, c("Subject", "Age_Old", "Gender_Male")])
dta = cbind(x, data.frame(Time = 0, Anger = 0, Contempt = 0, Disgust = 0, Fear = 0,
    Joy = 0, Sad = 0, Surprise = 0))

rm(x)

test.Neutral = data.frame()

for (i in seq(-1, 1, 0.1)) {
    x = dta
    x$Neutral = i
    test.Neutral = rbind(test.Neutral, x)
}

test.Neutral$predict = predict(mdl.08, test.Neutral, type = "prob")[, 2]

## Change Anger, all others constant
x = unique(mdl.08.train[, c("Subject", "Age_Old", "Gender_Male")])
dta = cbind(x, data.frame(Time = 0, Neutral = 0, Contempt = 0, Disgust = 0, Fear = 0,
    Joy = 0, Sad = 0, Surprise = 0))

rm(x)

test.Anger = data.frame()

for (i in seq(-1, 1, 0.1)) {
    x = dta
    x$Anger = i
    test.Anger = rbind(test.Anger, x)
}

test.Anger$predict = predict(mdl.08, test.Anger, type = "prob")[, 2]

## Change Contempt, all others constant
x = unique(mdl.08.train[, c("Subject", "Age_Old", "Gender_Male")])
dta = cbind(x, data.frame(Time = 0, Anger = 0, Neutral = 0, Contempt = 0, Disgust = 0,
    Fear = 0, Joy = 0, Sad = 0, Surprise = 0))

rm(x)

test.Contempt = data.frame()

for (i in seq(-1, 1, 0.1)) {
    x = dta
    x$Contempt = i
    test.Contempt = rbind(test.Contempt, x)
}

test.Contempt$predict = predict(mdl.08, test.Contempt, type = "prob")[, 2]

## Change Disgust, all others constant
x = unique(mdl.08.train[, c("Subject", "Age_Old", "Gender_Male")])
dta = cbind(x, data.frame(Time = 0, Anger = 0, Neutral = 0, Contempt = 0, Disgust = 0,
    Fear = 0, Joy = 0, Sad = 0, Surprise = 0))

rm(x)

test.Disgust = data.frame()

for (i in seq(-1, 1, 0.1)) {
    x = dta
    x$Disgust = i
    test.Disgust = rbind(test.Disgust, x)
}

test.Disgust$predict = predict(mdl.08, test.Disgust, type = "prob")[, 2]

## Change Fear, all others constant
x = unique(mdl.08.train[, c("Subject", "Age_Old", "Gender_Male")])
dta = cbind(x, data.frame(Time = 0, Anger = 0, Neutral = 0, Contempt = 0, Disgust = 0,
    Fear = 0, Joy = 0, Sad = 0, Surprise = 0))

rm(x)

test.Fear = data.frame()

for (i in seq(-1, 1, 0.1)) {
    x = dta
    x$Fear = i
    test.Fear = rbind(test.Fear, x)
}

test.Fear$predict = predict(mdl.08, test.Fear, type = "prob")[, 2]

## Change Joy, all others constant
x = unique(mdl.08.train[, c("Subject", "Age_Old", "Gender_Male")])
dta = cbind(x, data.frame(Time = 0, Anger = 0, Neutral = 0, Contempt = 0, Disgust = 0,
    Fear = 0, Joy = 0, Sad = 0, Surprise = 0))

rm(x)

test.Joy = data.frame()

for (i in seq(-1, 1, 0.1)) {
    x = dta
    x$Joy = i
    test.Joy = rbind(test.Joy, x)
}

test.Joy$predict = predict(mdl.08, test.Joy, type = "prob")[, 2]

## Change Sad, all others constant
x = unique(mdl.08.train[, c("Subject", "Age_Old", "Gender_Male")])
dta = cbind(x, data.frame(Time = 0, Anger = 0, Neutral = 0, Contempt = 0, Disgust = 0,
    Fear = 0, Joy = 0, Sad = 0, Surprise = 0))

rm(x)

test.Sad = data.frame()

for (i in seq(-1, 1, 0.1)) {
    x = dta
    x$Sad = i
    test.Sad = rbind(test.Sad, x)
}

test.Sad$predict = predict(mdl.08, test.Sad, type = "prob")[, 2]

## Change Surprise, all others constant
x = unique(mdl.08.train[, c("Subject", "Age_Old", "Gender_Male")])
dta = cbind(x, data.frame(Time = 0, Anger = 0, Neutral = 0, Contempt = 0, Disgust = 0,
    Fear = 0, Joy = 0, Sad = 0, Surprise = 0))

rm(x)

test.Surprise = data.frame()

for (i in seq(-1, 1, 0.1)) {
    x = dta
    x$Surprise = i
    test.Surprise = rbind(test.Surprise, x)
}

test.Surprise$predict = predict(mdl.08, test.Surprise, type = "prob")[, 2]




x1 = ggplot(test.Anger) +
  geom_boxplot(aes(x = Anger, y = predict, group = Anger), size = .25, alpha = 0.7, outlier.size = .5) +
  geom_smooth(aes(x = Anger, y = predict), size = .25, se = FALSE) +
  scale_y_continuous("P(Texting)")

x2 = ggplot(test.Contempt) +
  geom_boxplot(aes(x = Contempt, y = predict, group = Contempt), size = .25, alpha = 0.7, outlier.size = .5) +
  geom_smooth(aes(x = Contempt, y = predict), size = .25, se = FALSE) +
  scale_y_continuous("P(Texting)")

x3 = ggplot(test.Disgust) +
  geom_boxplot(aes(x = Disgust, y = predict, group = Disgust), size = .25, alpha = 0.7, outlier.size = .5) +
  geom_smooth(aes(x = Disgust, y = predict), size = .25, se = FALSE) +
  scale_y_continuous("P(Texting)")

x4 = ggplot(test.Fear) +
  geom_boxplot(aes(x = Fear, y = predict, group = Fear), size = .25, alpha = 0.7, outlier.size = .5) +
  geom_smooth(aes(x = Fear, y = predict), size = .25, se = FALSE) +
  scale_y_continuous("P(Texting)")

x5 = ggplot(test.Joy) +
  geom_boxplot(aes(x = Joy, y = predict, group = Joy), size = .25, alpha = 0.7, outlier.size = .5) +
  geom_smooth(aes(x = Joy, y = predict), size = .25, se = FALSE) +
  scale_y_continuous("P(Texting)")

x6 = ggplot(test.Sad) +
  geom_boxplot(aes(x = Sad, y = predict, group = Sad), size = .25, alpha = 0.7, outlier.size = .5) +
  geom_smooth(aes(x = Sad, y = predict), size = .25, se = FALSE) +
  scale_y_continuous("P(Texting)")

x7 = ggplot(test.Surprise) +
  geom_boxplot(aes(x = Surprise, y = predict, group = Surprise), size = .25, alpha = 0.7, outlier.size = .5) +
  geom_smooth(aes(x = Surprise, y = predict), size = .25, se = FALSE) +
  scale_y_continuous("P(Texting)")

x8 = ggplot(test.Neutral) +
  geom_boxplot(aes(x = Neutral, y = predict, group = Neutral), size = .25, alpha = 0.7, outlier.size = .5) +
  geom_smooth(aes(x = Neutral, y = predict), size = .25, se = FALSE) +
  scale_y_continuous("P(Texting)")


grid.arrange(x1, x2, x3, x4, x5, x6, x7, x8, nrow = 4)

```
\newpage

## Conclusions and Next Steps

It is not entirely surprising that the Nuetral emotion appears to be the most important variable. Neutral seems to capture the most variation across subjects. I expected the emotions Surprise, Joy, and Fear to have very little importance due to the fact that they are pretty much always at the baseline value. I think some additional investigation into the Surprise emotion is worth looking at to see if its importance can be explained. When looking at the probability model by emotion, broken out by Gender and Age, there appears to be some differences between the groups. My next task will be to look at this interaction and try to come up with some measurable differences between the groups.

The current model setup uses Subject T001 as the baseline that all other Subjects are compared to. This is a little difficult to explain in the output so I may also try to rearrange the model matrix so that there is no baseline Subject.

\newpage

#### Additional Plots

```{r h, echo=FALSE, comment=NA, warning=FALSE, message=FALSE}
x1 + facet_grid(Age_Old ~ Gender_Male, labeller = label_both)
x2 + facet_grid(Age_Old ~ Gender_Male, labeller = label_both)
x3 + facet_grid(Age_Old ~ Gender_Male, labeller = label_both)
x4 + facet_grid(Age_Old ~ Gender_Male, labeller = label_both)
x5 + facet_grid(Age_Old ~ Gender_Male, labeller = label_both)
x6 + facet_grid(Age_Old ~ Gender_Male, labeller = label_both)
x7 + facet_grid(Age_Old ~ Gender_Male, labeller = label_both)
x8 + facet_grid(Age_Old ~ Gender_Male, labeller = label_both)
```
