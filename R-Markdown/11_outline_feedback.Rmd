---
title: "Response to Feedback"
author: "Joseph Blubaugh"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document:
    fig_height: 3.5
    highlight: pygments
    latex_engine: xelatex
mainfont: DejaVu Sans Mono
sansfont: DejaVu Sans Mono
fontsize: 11pt
geometry: margin=1in
---

#### **Why not use ROC curves to graphically compare model performance?**

I did not think about this at all, but I think its a great idea.

![ROC Plot 1](../Presentation/Plots/ROC1.png)
![ROC Plot 2](../Presentation/Plots/ROC2.png)

##### Model Performance

| Model         | Data Processing      | Data Split     | MaxItr  | Size   | Decay   | Training | Testing  | AUC      |
|:--------------|:---------------------|:---------------|:--------|:-------|:--------|:---------|:---------|:---------|
| Model 1:      | Original             | 365 Split      | 100     | 50     | .20     | .760     | .676     | .734     |
| Model 2:      | Original             | Entire Sim     | 100     | 50     | .20     | .754     | .754     | .847     |
| Model 3:      | Differencing         | 365 Split      | 100     | 10     | .00     | .518     | .516     | .526     |
| Model 4:      | Differencing         | Entire Sim     | 100     | 25     | .10     | .572     | .571     | .637     |
| Model 5:      | Moving Avg           | 365 Split      | 100     | 10     | .00     | .503     | .502     | .527     |
| Model 6:      | Moving Avg           | Entire Sim     | 100     | 10     | .00     | .528     | .528     | .544     |
| Model 7:      | 1/2 Sec Cut          | 365 Split      | 100     | 50     | .10     | .820     | .698     | .761     |
| **Model 8:**  | **1/2 Sec Cut**      | **Entire Sim** | **100** | **50** | **.20** | **.788** | **.779** | **.868** |
| Model 9:      | 1/2 Sec Diff         | 365 Split      | 100     | 50     | .10     | .633     | .602     | .650     |
| Model 10:     | 1/2 Sec Diff         | Entire Sim     | 100     | 50     | .20     | .682     | .622     | .681     |
| Model 11:     | 1/2 Sec Cut Stat     | 365 Split      | 100     | 50     | .10     | .846     | .716     | .781     |
| **Model 12:** | **1/2 Sec Cut Stat** | **Entire Sim** | **100** | **50** | **.20** | **.820** | **.803** | **.891** |

\newpage

##### Further Training Best Models

| Model        | Data Processing  | Data Split     | MaxItr   | Size   | Decay   | Training | Testing  | AUC      |
|:-------------|:-----------------|:---------------|:---------|:-------|:--------|:---------|:---------|:---------|
| Model 8:     | 1/2 Sec Cut      | Entire Sim     | 250      | 50     | .10     | .816     | .804     | .893     |
| Model 8:     | 1/2 Sec Cut      | Entire Sim     | 500      | 50     | .10     | .828     | .810     | .899     |
| **Model 8:** | **1/2 Sec Cut**  | **Entire Sim** | **1000** | **50** | **.10** | **.842** | **.820** | **.906** |
| Model 12:    | 1/2 Sec Cut Stat | Entire Sim     | 250      | 50     | .10     | .858     | .823     | .906     |
| Model 12:    | 1/2 Sec Cut Stat | Entire Sim     | 500      | 50     | .20     | .864     | .823     | .907     |
| Model 12:    | 1/2 Sec Cut Stat | Entire Sim     | 1000     | 50     | .10     | .871     | .824     | .908     |



##### Balanced Accuracy by Subject


|                | T022  | T007  | T086  | T006  | T018  | T083  | T035  | T076  | T081  | T064  | T020  | T012  | T074  | T088  | T013  | T009  | T032  | T003  | T011  | T080  | **Top 20** |
|:---------------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:-----------|
| **Train**      | 0.982 | 0.920 | 0.956 | 0.941 | 0.935 | 0.956 | 0.951 | 0.949 | 0.927 | 0.921 | 0.932 | 0.923 | 0.923 | 0.938 | 0.896 | 0.911 | 0.915 | 0.899 | 0.902 | 0.896 | **.943**   |
| **Test**       | 0.975 | 0.951 | 0.950 | 0.941 | 0.936 | 0.930 | 0.928 | 0.927 | 0.922 | 0.921 | 0.919 | 0.911 | 0.907 | 0.900 | 0.898 | 0.897 | 0.892 | 0.884 | 0.874 | 0.872 | **.938**   |
| **GenderMale** | 0     | 1     | 0     | 0     | 0     | 1     | 0     | 1     | 1     | 0     | 0     | 0     | 1     | 1     | 1     | 1     | 1     | 1     | 1     | 0     | **11**     |
| **AgeOld**     | 0     | 0     | 1     | 0     | 0     | 1     | 1     | 1     | 0     | 0     | 0     | 0     | 1     | 1     | 0     | 0     | 1     | 0     | 0     | 0     | **7**      |



|                | T016  | T005  | T060  | T044  | T015  | T008  | T079  | T073  | T039  | T082  | T010  | T066  | T029  | T046  | T024  | T051  | T017  | T042  | T001  | T061  | **Mid 20** |
|:---------------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:-----------|
| **Train**      | 0.893 | 0.865 | 0.909 | 0.886 | 0.864 | 0.868 | 0.893 | 0.855 | 0.869 | 0.869 | 0.849 | 0.849 | 0.813 | 0.857 | 0.810 | 0.856 | 0.807 | 0.843 | 0.851 | 0.793 | **.854**   |
| **Test**       | 0.862 | 0.861 | 0.856 | 0.849 | 0.847 | 0.841 | 0.838 | 0.834 | 0.833 | 0.832 | 0.826 | 0.813 | 0.812 | 0.810 | 0.808 | 0.806 | 0.806 | 0.803 | 0.800 | 0.787 | **.826**   |
| **GenderMale** | 0     | 1     | 0     | 1     | 1     | 0     | 0     | 0     | 0     | 1     | 0     | 0     | 0     | 1     | 0     | 1     | 1     | 1     | 1     | 1     | **10**     |
| **AgeOld**     | 0     | 0     | 0     | 1     | 0     | 0     | 0     | 1     | 1     | 0     | 0     | 0     | 1     | 1     | 0     | 1     | 0     | 1     | 0     | 0     | **7**      |



|                | T084  | T077  | T036  | T031  | T033  | T040  | T021  | T014  | T019  | T004  | T002  | T023  | T054  | T041  | T034  | T047  | T025  | T038  | T027  | **Bot 19** |
|:---------------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:------|:-----------|
| **Train**      | 0.806 | 0.794 | 0.789 | 0.810 | 0.768 | 0.782 | 0.802 | 0.815 | 0.742 | 0.728 | 0.717 | 0.748 | 0.741 | 0.698 | 0.688 | 0.703 | 0.699 | 0.559 | 0.540 | **.733**   |
| **Test**       | 0.780 | 0.770 | 0.766 | 0.765 | 0.751 | 0.750 | 0.748 | 0.748 | 0.734 | 0.721 | 0.719 | 0.706 | 0.698 | 0.697 | 0.677 | 0.670 | 0.660 | 0.532 | 0.514 | **.705**   |
| **GenderMale** | 1     | 0     | 1     | 0     | 0     | 1     | 1     | 0     | 1     | 0     | 0     | 1     | 1     | 0     | 1     | 0     | 0     | 1     | 0     | **9**      |
| **AgeOld**     | 1     | 1     | 1     | 1     | 1     | 1     | 0     | 0     | 0     | 0     | 0     | 0     | 1     | 1     | 1     | 1     | 1     | 1     | 1     | **13**     |



###### Summarised Accuracy by Subject

|        | % Male | % Female | % Old | % Young | % Young Male | % Young Female | % Old Male | % Old Female |
|:-------|:------:|:---------|:-----:|:--------|:-------------|:---------------|:----------:|:------------:|
| Top 20 | 36.6%  | 31.0%    | 25.9% | 40.6%   |              |                |   35.7%    |    16.7%     |
| Mid 20 | 33.3%  | 34.5%    | 25.9% | 40.6%   |              |                |   21.4%    |    25.0%     |
| Bot 19 | 30.0%  | 34.5%    | 48.1% | 18.7%   |              |                |   42.9%    |    58.3%     |


* 15 of 59 had testing performance > 90%
* 40 of 59 had testing performance > 80%
*  3 of 59 had testing performance < 70%
*  6 of the 7 worst performaning Subjects were Old (4 Male, 3 Female)
* The 15 top performing Subjects (7 Male, 5 Old)


#### **Are you trying to detect whether texting occurred during a given interval or a single observation?**

Initially I was looking at individual observations, but as I tried different approaches I ended up aggregating the data and found those models to be the best performing.

#### **Baselining: What is the purpose?**

My goal was to remove the effect of the simulation from the texting trial. The simulations involved active traffic and a detour which each driver had to navigate. Since trial 4 was identical to trials 5, 6, and 7 minus the events that took place, it seemed like the natural choice.

#### **Why do you say there was evidence that one size fits all model will not work?**

At this stage I was aggregating data at the subject level by event and I think the range of values were all over the place which prevented the models I tried from being any good. I proposed that adding a factor variable for subject would allow me to model individuals driving behavior that might be more successful at detecting texting.

#### **On Time omission -- why not include lagged variables?**

In retrospect this probably would have been a good thing to try. The farthest I went looking into time series was trying differencing which ended up being the worst of all of the methods I experimented with.

#### **Does including age and gender as well as Subject in the model cause problems due to collinerity?**

I mainly included included age and gender so that there were parameters availble to account for universal age and gender effects, if there were any. I dont think there is an issue with collinerity because Im not inferring anything about the weights (coefficients).

#### **Exmplain (sens+spec)/2**

I presented this in a more complicated way than was neccessary. It turns out that the formula is equivalent to the "total percentage correct". I initially set it up that way incase I wanted to penalize false positives or negatives more. I had a multivariate course last year that did this. I never ended up doing that so I will eliminate that metric and just represent it as percentage correct.

#### **Why move 60/40 to 50/50?**

Only because the initial aggregate data I created had very few observations so I was just trying to have more data to train on. There was no real scientific reason for this. I may cut this exploratory section out entirely.

#### **Less on interpreting nnet weights**

Agreed. I have pretty much concluded that trying to extract and interpret weights on large nueral nets is very tricky and not very helpful.


#### **Reduce or cut material on initial analysis and aggregated data**?

Agreed.
